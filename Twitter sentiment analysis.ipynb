{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/z1xtr/Downloads/Naive_bayes/Naive_bayes/tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = df.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355204a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce68d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = '[{}]'.format(re.escape(string.punctuation))\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efda077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stw = nltk.corpus.stopwords.words('english')\n",
    "print(stw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b005e",
   "metadata": {},
   "source": [
    "# Defining a function for text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# converting the text into all lower cases\n",
    "\n",
    "def text_lower(text):\n",
    "    tlower = pd.Series([sents.lower() for sents in text])\n",
    "    return tlower\n",
    "\n",
    "# word tokenization\n",
    "\n",
    "def word_tkns(text):\n",
    "    tkns = pd.Series([nltk.tokenize.word_tokenize(sents) for sents in text])\n",
    "    return tkns\n",
    "\n",
    "# removing punctuation\n",
    "\n",
    "def punct_removal(text):\n",
    "    punct_pattern = '[{}]'.format(re.escape(string.punctuation))\n",
    "    regex_pattern = re.compile(punct_pattern)\n",
    "    clean_sents1= pd.Series([list(filter(None, [regex_pattern.sub('',  x) for x in sents])) for sents in text])\n",
    "    return clean_sents1\n",
    "\n",
    "# stopword removal\n",
    "# remember to not remove negations in the case of \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    clean_sents2 = pd.Series([[x for x in sents if x not in stopwords] for sents in text])\n",
    "    return clean_sents2\n",
    "\n",
    "\n",
    "## correcting words like spelling mistakes, repeated characters, etc. \n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def remove_repeated_character(text):\n",
    "    \n",
    "    # pattern that occur twice among other characters\n",
    "    pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    replacement = r'\\1\\2\\3'\n",
    "    \n",
    "    def replace(tkn):\n",
    "        \n",
    "        # check semantically correct word if not replace and check again\n",
    "        if wn.synsets(tkn):\n",
    "            return tkn\n",
    "        \n",
    "        tkn_c = pattern.sub(replacement, tkn)\n",
    "        \n",
    "        # recursive call\n",
    "        return replace(tkn_c) if tkn_c != tkn else tkn_c\n",
    "    \n",
    "     # correct each token \n",
    "    token_c = pd.Series([[replace(tn) for tn in sents] for sents in text])\n",
    "    return token_c\n",
    "\n",
    "## PoS tagging\n",
    "\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    tkn_tagged = pd.Series([nltk.pos_tag(sents, tagset= 'universal') for sents in text])\n",
    "    \n",
    "    def penn_to_wn(ptag):\n",
    "        \n",
    "        if ptag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        if ptag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        if ptag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        if ptag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None \n",
    "    \n",
    "    tkn_tagged_wn = pd.Series([[(token, penn_to_wn(tag)) for token, tag in sents] for sents in tkn_tagged])\n",
    "    return tkn_tagged_wn\n",
    "                             \n",
    "## Lemmatization using WordNetLemmatizer\n",
    "\n",
    "def wnl(text):\n",
    "    wnlemmatizer = WordNetLemmatizer()\n",
    "    stemmed_text = pd.Series([[wnlemmatizer.lemmatize(words, tag) if tag else wnlemmatizer.lemmatize(words) for words, tag in sents ] for sents in text])\n",
    "    return stemmed_text                          \n",
    "\n",
    "# Converting earch element in the series from a list to string\n",
    "\n",
    "def list_to_string(text):\n",
    "    str_series = pd.Series([' '.join(sents) for sents in text])\n",
    "    return str_series\n",
    "    \n",
    "# Defining a function for text pre-processing\n",
    "\n",
    "def preprocessed_text(text):\n",
    "    \n",
    "    tl = text_lower(text)\n",
    "    wtkns = word_tkns(tl)\n",
    "    punc_removal = punct_removal(wtkns)\n",
    "    remove_stw = remove_stopwords(punc_removal)\n",
    "    removing_extra_characters = remove_repeated_character(remove_stw) \n",
    "    wn_tagged = pos_tag_text(removing_extra_characters)\n",
    "    lemmatized_text = wnl(wn_tagged)\n",
    "    preprocessed_text= list_to_string(lemmatized_text)\n",
    "    return preprocessed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e615070",
   "metadata": {},
   "source": [
    "## Testing the function preprocessed_text(text) on tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_c = preprocessed_text(tweet)\n",
    "print(tweet_c[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09cf7f",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION USING TF-IDF MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d928ae",
   "metadata": {},
   "source": [
    "### Creating train and test partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc526be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tweet_train, tweet_test, label_train, label_test = train_test_split(tweet_c, label, test_size= 0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccf2df",
   "metadata": {},
   "source": [
    "# Extracting features using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# creating an instance of TfidfVectorizer\n",
    "tfidfvectorizer = TfidfVectorizer(norm='l2', smooth_idf=True, use_idf=True)\n",
    "\n",
    "# using fit_transform method to extract the features from tweet-train\n",
    "features_train_t = tfidfvectorizer.fit_transform(tweet_train)\n",
    "features_train_tm = features_train_t.toarray()\n",
    "\n",
    "# tranforming the tweet-test\n",
    "features_test_t = tfidfvectorizer.transform(tweet_test)\n",
    "features_test_tm = features_test_t.toarray()\n",
    "\n",
    "# get feature names\n",
    "feature_names_t = tfidfvectorizer.get_feature_names_out()\n",
    "print(feature_names_t[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c319254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "df_train_t = pd.DataFrame(data=features_train_tm, columns= feature_names_t)\n",
    "df_test_t = pd.DataFrame(data=features_test_tm, columns= feature_names_t)\n",
    "\n",
    "print(df_train_t.iloc[0:5, 0:10])\n",
    "print(df_test_t.iloc[0:5, 0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d2822",
   "metadata": {},
   "source": [
    "## Implement Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b9a78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create an instance of MultinomialNB()\n",
    "multinomialnb = MultinomialNB()\n",
    "\n",
    "# train the model\n",
    "multinomialnb.fit(features_train_t, label_train)\n",
    "# make_predictions\n",
    "predictions_t = multinomialnb.predict(features_test_t)\n",
    "\n",
    "# check accuracy\n",
    "as_t = accuracy_score(label_test, predictions_t)\n",
    "print('accuracy score:', as_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a992ac8",
   "metadata": {},
   "source": [
    "# classification matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification matrix\n",
    "from sklearn import metrics\n",
    "cm_multiNB = pd.DataFrame(data= metrics.confusion_matrix(label_test, predictions_t))\n",
    "cm_multiNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85715a78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total 359 wrong prediction for class 1\n",
    "df_predictions_check = pd.DataFrame({'tweet' : tweet_test, 'predicted_label' : predictions_t, 'label_t' : label_test})\n",
    "df_predictions_check[df_predictions_check['label_t'] == 1][df_predictions_check['predicted_label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d959f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[7539]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dab849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[554]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf9a47",
   "metadata": {},
   "source": [
    "## Implement Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "bernoullinb2 = BernoulliNB()\n",
    "bernoullinb2.fit(features_train_t, label_train)\n",
    "predictions_bnb2 = bernoullinb2.predict(features_test_t)\n",
    "\n",
    "as_bnb2 = accuracy_score(label_test, predictions_bnb2)\n",
    "print('accuracy score:', as_bnb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98cae9",
   "metadata": {},
   "source": [
    "## classification matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99090e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data= metrics.confusion_matrix(label_test, predictions_bnb2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f75ba",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b619a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "svm1 = SGDClassifier()\n",
    "svm1.fit(features_train_t, label_train)\n",
    "predictions_svm1 = svm1.predict(features_test_t)\n",
    "\n",
    "print('accuracy score:', metrics.accuracy_score(label_test, predictions_svm1), '\\nprecision score:', metrics.precision_score(label_test, predictions_svm1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead40c83",
   "metadata": {},
   "source": [
    "## classification matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data= metrics.confusion_matrix(label_test, predictions_svm1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
